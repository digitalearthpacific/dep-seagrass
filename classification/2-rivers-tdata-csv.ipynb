{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718f54c3-6d20-4ed0-9752-e1751e6b84e5",
   "metadata": {},
   "source": [
    "# Notebook 1 prepare postcard and load data to csv\n",
    "\n",
    "The objective of this notebook is to prepare a geomad postcard for your AOI (masking, scaling and loading additional band ratios and spectral indices) and sampling all the datasets into a csv based on your training data geodataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887498ee",
   "metadata": {},
   "source": [
    "## Step 1.1: Configure the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98bda661-ae78-4f82-82b3-e68a5aa32d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from dask.distributed import Client as DaskClient\n",
    "from odc.stac import configure_s3_access, load\n",
    "from pystac.client import Client\n",
    "from utils import calculate_band_indices, scale, texture\n",
    "from masking import all_masks\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2edc9da-3387-495f-b7bc-5e3210259076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload scripts and imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c82e096b-bf28-4ce2-9c2a-523430237278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "folder = Path(\"training-data/turbidity/\").expanduser()\n",
    "\n",
    "files = folder.glob(\"*.geojson\")\n",
    "\n",
    "# geodataframes = []\n",
    "# for f in files:\n",
    "#     # gdf = gpd.read_file(f)\n",
    "#     # Add the file name as a column\n",
    "#     gdf = gpd.read_file(f).to_crs(\"epsg:4326\").copy()\n",
    "#     gdf[\"site_name\"] = f.name.rstrip(\"_turbid.geojson\")\n",
    "#     geodataframes.append(gdf.to_crs(\"epsg:4326\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3013a252-2067-48de-9636-0dd32adbfcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process png_gulf_region_west.geojson: name 'geodataframes' is not defined\n",
      "Failed to process fj_labasa_turbid.geojson: name 'geodataframes' is not defined\n",
      "Failed to process png_gulf_central_turbid.geojson: name 'geodataframes' is not defined\n",
      "Failed to process fj_sigatoka_turbid.geojson: name 'geodataframes' is not defined\n",
      "Failed to process png_east_sepik_turbid.geojson: name 'geodataframes' is not defined\n",
      "Failed to process sols_western_turbid.geojson: name 'geodataframes' is not defined\n",
      "Failed to process sols_mataniko_turbid.geojson: name 'geodataframes' is not defined\n",
      "Failed to process png_fly_river.geojson: name 'geodataframes' is not defined\n",
      "Failed to process png_gulf_central.geojson: name 'geodataframes' is not defined\n",
      "Failed to process png_merauke.geojson: name 'geodataframes' is not defined\n",
      "Failed to process png_fly_river_turbid.geojson: name 'geodataframes' is not defined\n",
      "Failed to process png_west_sepik_turbid.geojson: name 'geodataframes' is not defined\n",
      "Failed to process sols_east_mataniko_turbid.geojson: name 'geodataframes' is not defined\n",
      "Failed to process vu_malampa_turbid.geojson: name 'geodataframes' is not defined\n",
      "Failed to process png_gulf_west.geojson: name 'geodataframes' is not defined\n",
      "Failed to process wp_merauke_turbid.geojson: name 'geodataframes' is not defined\n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "    try:\n",
    "        # --- METHOD 1: Using .copy() to break the chain ---\n",
    "        # Read the file, transform the CRS, and explicitly create a new copy.\n",
    "        gdf_copy_method = gpd.read_file(f).to_crs(\"epsg:4326\").copy()\n",
    "        \n",
    "        # Now, you can safely add a new column to the copy.\n",
    "        gdf_copy_method[\"site_name\"] = f.name.rstrip(\"_turbid.geojson\")\n",
    "        \n",
    "        geodataframes.append(gdf_copy_method)\n",
    "\n",
    "        # --- METHOD 2: Using .loc to safely assign a value ---\n",
    "        # This method is often preferred as it is the \"correct\" way to do this in Pandas.\n",
    "        gdf_loc_method = gpd.read_file(f).to_crs(\"epsg:4326\")\n",
    "        \n",
    "        # We use .loc to set the entire new column, which prevents the warning.\n",
    "        gdf_loc_method.loc[:, \"site_name\"] = f.name.rstrip(\"_turbid.geojson\")\n",
    "        \n",
    "        geodataframes.append(gdf_loc_method)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {f.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fcde31-7567-4eff-95b3-6900559b5dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226be5a8-53b1-4746-aa10-4f8880eb1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf = pd.concat(geodataframes, ignore_index=True)\n",
    "merged_gdf[\"coastal_class\"] = merged_gdf[\"coastal_class\"].fillna(merged_gdf[\"observed\"])\n",
    "merged_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03004c2e-d95f-4420-afbd-a42f704e98a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf = merged_gdf.drop(columns=[\"id\"])\n",
    "merged_gdf.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac87fb-c7ea-487a-b9c6-d0a5cdb9f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = merged_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f4a84-0479-41d8-b985-d4d2e4ec7a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = tdata.rename(columns={'Class': 'coastal_class', 'coastal_cl': 'coastal_class', 'class': 'coastal_class'}, inplace=False)\n",
    "print(tdata.columns.unique) \n",
    "tdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6563f2-23db-4e04-bf74-edc2f9c286cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your DataFrame has three columns all called 'coastal_class'\n",
    "# we select them by filtering on column name\n",
    "coastal_class = tdata.loc[:, tdata.columns == 'coastal_class']\n",
    " \n",
    "# Merge them into one column (take the first non-null value per row)\n",
    "tdata['coastal_class'] = coastal_class.bfill(axis=1).iloc[:, 0]\n",
    " \n",
    "# Drop the duplicate ones (keeping only the new single column)\n",
    "tdata = tdata.loc[:, ~tdata.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8860c5ac-293d-48cf-a640-74d89b319e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be91b8ba-0f41-4b53-8b79-8906973134ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tdata.columns.unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f08aa9-2f6a-4d77-8b60-1137e3131669",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tdata['coastal_class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b4ab5-cb4b-46e6-9bc9-23526b4b0b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tdata = training_data_all \n",
    "tdata['observed'] = tdata['coastal_class'].replace('Land', 'land') #: This method is called on the Series (the column) and replaces all occurrences of 'old_value' with 'new_value'.\n",
    "tdata['observed'] = tdata['coastal_class'].replace('Deeps', 'deeps') #: This method is called on the Series (the column) and replaces all occurrences of 'old_value' with 'new_value'.\n",
    "tdata['observed'] = tdata['coastal_class'].replace('Turbidity', 'sediment') #: This method is called on the Series (the column) and replaces all occurrences of 'old_value' with 'new_value'.\n",
    "# print(tdata.columns.unique)\n",
    "tdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56097b7d-b4e4-4808-bad8-3c032ba87092",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4aee6813-4060-43c8-9b24-c0c9fc948130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['geometry', 'site_name', 'coastal_class', 'cc_id', 'observed'], dtype='object')\n",
      "observed\n",
      "1.0     194\n",
      "8.0      85\n",
      "10.0     32\n",
      "Name: count, dtype: int64\n",
      "total gps points 3046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/geopandas/geodataframe.py:1968: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "# Define two lists\n",
    "old_categories = ['sediment', 'sand', 'rubble', 'seagrass', 'algae', 'coral', 'rock', 'deeps', 'mangrove', 'land']\n",
    "new_groups = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Create the mapping dictionary\n",
    "category_mapping = dict(zip(old_categories, new_groups))\n",
    "\n",
    "# Apply mapping\n",
    "# tdata['observed_id'] = tdata['observed'].map(category_mapping)\n",
    "tdata['observed'] = tdata['coastal_class'].map(category_mapping)\n",
    "\n",
    "# Print the result\n",
    "print(tdata.columns)\n",
    "\n",
    "print(tdata['observed'].value_counts())\n",
    "print('total gps points',(len(tdata)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e841ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data_all = gpd.read_file(\"training-data/all_tdata_082025.geojson\")\n",
    "# training_data_all.explore(column=\"coastal_class\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f556609",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtraining_data_all\u001b[49m.head()\n",
      "\u001b[31mNameError\u001b[39m: name 'training_data_all' is not defined"
     ]
    }
   ],
   "source": [
    "training_data_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac040552-54eb-4110-980d-2aa59a7242cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_gdf = pd.concat(geodataframes, ignore_index=True)\n",
    "# tdata[\"coastal_class\"] = tdata[\"coastal_class\"].fillna(tdata[\"observed\"])\n",
    "\n",
    "# tdata = tdata.drop(columns=[\"observed\", \"fid\"])\n",
    "tdata.to_file(\"all_tdata_10092025.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd626b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the cc_id and coastal_class to find any mistakes\n",
    "test = training_data_all[\"cc_id\"].astype(str) + \"_\" + training_data_all[\"coastal_class\"].astype(str)\n",
    "\n",
    "# Show unique combinations with count\n",
    "unique = test.value_counts()\n",
    "unique.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb62e5e-32bf-4ccd-8ac1-8e7c95102d5a",
   "metadata": {},
   "source": [
    "## Step 1.2: Configure STAC access and search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975ca2ac-2484-4d63-8f54-e7370dd8764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = \"https://stac.digitalearthpacific.org\"\n",
    "client = Client.open(catalog)\n",
    "\n",
    "configure_s3_access(aws_unsigned=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5117d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_site(site_name, training_data, mask=True):\n",
    "    site_name = site_name\n",
    "    extent = training_data.total_bounds\n",
    "    datetime = \"2024\"\n",
    "\n",
    "    items = client.search(\n",
    "        collections=[\"dep_s2_geomad\"], bbox=extent, datetime=datetime\n",
    "    ).item_collection()\n",
    "\n",
    "    print(f\"Found {len(items)} items in for {datetime} for {site_name}\")\n",
    "\n",
    "    data = load(\n",
    "        items,\n",
    "        bbox=extent,\n",
    "        chunks=dict(x=2024, y=2024),\n",
    "        fail_on_error=False,\n",
    "        datetime=datetime,\n",
    "        measurements=[\n",
    "            \"nir\",\n",
    "            \"red\",\n",
    "            \"blue\",\n",
    "            \"green\",\n",
    "            \"emad\",\n",
    "            \"smad\",\n",
    "            \"bcmad\",\n",
    "            \"green\",\n",
    "            \"nir08\",\n",
    "            \"nir09\",\n",
    "            \"swir16\",\n",
    "            \"swir22\",\n",
    "            \"coastal\",\n",
    "            \"rededge1\",\n",
    "            \"rededge2\",\n",
    "            \"rededge3\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    scaled_data = scale(data).squeeze(drop=True)\n",
    "    loaded_data = scaled_data.compute()\n",
    "\n",
    "    print(\"Loaded data into memory\")\n",
    "\n",
    "    data_indices = calculate_band_indices(loaded_data)\n",
    "    texture_data = texture(data_indices.blue, levels=32).compute()\n",
    "    combined_data = xr.merge([data_indices, texture_data])\n",
    "    if mask:\n",
    "        final_data = all_masks(combined_data)\n",
    "    else:\n",
    "        final_data = combined_data\n",
    "\n",
    "    print(\"Finished preparing indices and masking data\")\n",
    "\n",
    "    # Reproject training data to the GeoMAD CRS and convert to xarray\n",
    "    training_reprojected = training_data.to_crs(final_data.odc.crs)\n",
    "    training_da = training_reprojected.assign(\n",
    "        x=training_reprojected.geometry.x, y=training_reprojected.geometry.y\n",
    "    ).to_xarray()\n",
    "\n",
    "    # Extract training values from the masked dataset\n",
    "    training_values = (\n",
    "        final_data.sel(training_da[[\"x\", \"y\"]], method=\"nearest\")\n",
    "        .squeeze()\n",
    "        .compute()\n",
    "        .to_pandas()\n",
    "    )\n",
    "\n",
    "    print(\"Finished extracting training values\")\n",
    "\n",
    "    # Join the training data with the extracted values and remove unnecessary columns\n",
    "    training_array = pd.concat([training_data[\"cc_id\"], training_values], axis=1)\n",
    "\n",
    "    # Drop rows where there was no data available\n",
    "    training_array = training_array.dropna()\n",
    "\n",
    "    # training_array.drop(columns=[\"spatial_ref\", \"x\", \"y\"], inplace=True)\n",
    "\n",
    "    # Preview our resulting training array\n",
    "    return training_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a580b2-f7cf-444c-bcd9-fb87088c4f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked = True\n",
    "\n",
    "all_training_data = []\n",
    "with DaskClient(n_workers=2, threads_per_worker=16, memory_limit='12GB') as dc:\n",
    "    print(dc.dashboard_link)\n",
    "    for site_name, group in training_data_all.groupby(\"site_name\"):\n",
    "        print(f\"PROCESSING: {site_name}\")\n",
    "        extent = group.total_bounds\n",
    "        training_data = process_site(site_name, group, mask=masked)\n",
    "        all_training_data.append(training_data)\n",
    "        training_data.to_csv(f\"training-data/{site_name}{'' if masked else '-unmasked'}-training.csv\", index=False)\n",
    "\n",
    "# Combine all training data into a single DataFrame\n",
    "combined_training_data = pd.concat(all_training_data, axis=0)\n",
    "print(len(combined_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the combined training data\n",
    "combined_training_data.to_csv(f\"training-data/combined{'' if masked else '-unmasked'}-training.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
