{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718f54c3-6d20-4ed0-9752-e1751e6b84e5",
   "metadata": {},
   "source": [
    "# Notebook 1 prepare postcard and load data to csv\n",
    "\n",
    "The objective of this notebook is to prepare a geomad postcard for your AOI (masking, scaling and loading additional band ratios and spectral indices) and sampling all the datasets into a csv based on your training data geodataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887498ee",
   "metadata": {},
   "source": [
    "## Step 1.1: Configure the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bda661-ae78-4f82-82b3-e68a5aa32d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from dask.distributed import Client as DaskClient\n",
    "from odc.stac import configure_s3_access, load\n",
    "from pystac.client import Client\n",
    "from utils import calculate_band_indices, scale, texture\n",
    "from masking import all_masks\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2edc9da-3387-495f-b7bc-5e3210259076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload scripts and imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e096b-bf28-4ce2-9c2a-523430237278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# folder = Path(\"~/data/seagrass\").expanduser()\n",
    "\n",
    "# files = folder.glob(\"*.geojson\")\n",
    "\n",
    "# geodataframes = []\n",
    "# for f in files:\n",
    "#     gdf = gpd.read_file(f)\n",
    "#     # Add the file name as a column\n",
    "#     gdf[\"site_name\"] = f.name.replace(\"_postcard.geojson\", \"\")\n",
    "#     geodataframes.append(gdf.to_crs(\"epsg:4326\"))\n",
    "\n",
    "# merged_gdf = pd.concat(geodataframes, ignore_index=True)\n",
    "# merged_gdf[\"coastal_class\"] = merged_gdf[\"coastal_class\"].fillna(merged_gdf[\"observed\"])\n",
    "\n",
    "# merged_gdf = merged_gdf.drop(columns=[\"observed\", \"date\", \"observed_id\", \"id\", \"fid\", \"uuid\"])\n",
    "\n",
    "# # Update anything that is `algae` so that cc_id is 11\n",
    "# merged_gdf.loc[merged_gdf[\"coastal_class\"] == \"algae\", \"cc_id\"] = 11\n",
    "# # Update anything that is `seagrass` to be class 4\n",
    "# merged_gdf.loc[merged_gdf[\"coastal_class\"] == \"seagrass\", \"cc_id\"] = 4\n",
    "# # Land is 10\n",
    "# merged_gdf.loc[merged_gdf[\"coastal_class\"] == \"land\", \"cc_id\"] = 10\n",
    "# # Deeps is 12\n",
    "# merged_gdf.loc[merged_gdf[\"coastal_class\"] == \"deeps\", \"cc_id\"] = 12\n",
    "# # Sediment is 1\n",
    "# merged_gdf.loc[merged_gdf[\"coastal_class\"] == \"sediment\", \"cc_id\"] = 1\n",
    "# # Seaweed is 5\n",
    "# merged_gdf.loc[merged_gdf[\"coastal_class\"] == \"seaweed\", \"cc_id\"] = 5\n",
    "# # Rock is 13\n",
    "# merged_gdf.loc[merged_gdf[\"coastal_class\"] == \"rock\", \"cc_id\"] = 13\n",
    "# # Sand is 2\n",
    "# merged_gdf.loc[merged_gdf[\"coastal_class\"] == \"sand\", \"cc_id\"] = 2\n",
    "# # Coral is 6\n",
    "# merged_gdf.loc[merged_gdf[\"coastal_class\"] == \"coral\", \"cc_id\"] = 6\n",
    "# # Mangrove is 9\n",
    "# merged_gdf.loc[merged_gdf[\"coastal_class\"] == \"mangrove\", \"cc_id\"] = 9\n",
    "\n",
    "# merged_gdf.to_file(\"training_all.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e841ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_all = gpd.read_file(\"~/data/seagrass/all_field_tdata_11092025.geojson\")\n",
    "# tulagi = gpd.read_file(\"~/data/seagrass/sols_tulagi_ws_postcard.geojson\")\n",
    "# tulagi[\"file_name\"] = \"sols_tulagi_ws\"\n",
    "# tulagi[\"coastal_class\"] = tulagi[\"observed\"]\n",
    "# tulagi.drop(columns=[\"observed\", \"id\"], inplace=True)\n",
    "\n",
    "rivers = gpd.read_file(\"~/data/seagrass/rivers_tdata_11092025_postcard.geojson\")\n",
    "rivers[\"file_name\"] = rivers[\"site_name\"]\n",
    "\n",
    "training_data_all = pd.concat([training_data_all, rivers], ignore_index=True)\n",
    "\n",
    "training_data_all.drop(columns=[\"id\", \"index\", \"site_name\"], inplace=True)\n",
    "training_data_all.rename(columns={\"file_name\": \"site_name\"}, inplace=True)\n",
    "\n",
    "training_data_all.explore(column=\"site_name\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f556609",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46934f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111accd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_all = gpd.read_file(\"all_data.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd626b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the cc_id and coastal_class to find any mistakes\n",
    "test = training_data_all[\"cc_id\"].astype(str) + \"_\" + training_data_all[\"observed\"].astype(str)\n",
    "\n",
    "# Show unique combinations with count\n",
    "unique = test.value_counts()\n",
    "unique.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb62e5e-32bf-4ccd-8ac1-8e7c95102d5a",
   "metadata": {},
   "source": [
    "## Step 1.2: Configure STAC access and search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975ca2ac-2484-4d63-8f54-e7370dd8764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = \"https://stac.digitalearthpacific.org\"\n",
    "client = Client.open(catalog)\n",
    "\n",
    "configure_s3_access(aws_unsigned=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5117d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_site(site_name, training_data, mask=True):\n",
    "    site_name = site_name\n",
    "    extent = training_data.total_bounds\n",
    "    datetime = \"2024\"\n",
    "\n",
    "    items = client.search(\n",
    "        collections=[\"dep_s2_geomad\"], bbox=extent, datetime=datetime\n",
    "    ).item_collection()\n",
    "\n",
    "    print(f\"Found {len(items)} items in for {datetime} for {site_name}\")\n",
    "\n",
    "    data = load(\n",
    "        items,\n",
    "        bbox=extent,\n",
    "        chunks=dict(x=2024, y=2024),\n",
    "        fail_on_error=False,\n",
    "        datetime=datetime,\n",
    "        measurements=[\n",
    "            \"nir\",\n",
    "            \"red\",\n",
    "            \"blue\",\n",
    "            \"green\",\n",
    "            \"emad\",\n",
    "            \"smad\",\n",
    "            \"bcmad\",\n",
    "            \"green\",\n",
    "            \"nir08\",\n",
    "            \"nir09\",\n",
    "            \"swir16\",\n",
    "            \"swir22\",\n",
    "            \"coastal\",\n",
    "            \"rededge1\",\n",
    "            \"rededge2\",\n",
    "            \"rededge3\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    print(f\"Loading array that is {data.dims}\")\n",
    "\n",
    "    scaled_data = scale(data).squeeze(drop=True)\n",
    "    loaded_data = scaled_data.compute()\n",
    "\n",
    "    print(\"Loaded data into memory\")\n",
    "\n",
    "    data_indices = calculate_band_indices(loaded_data)\n",
    "    texture_data = texture(data_indices.blue, levels=32).compute()\n",
    "    combined_data = xr.merge([data_indices, texture_data])\n",
    "    if mask:\n",
    "        final_data = all_masks(combined_data)\n",
    "    else:\n",
    "        final_data = combined_data\n",
    "\n",
    "    print(\"Finished preparing indices and masking data\")\n",
    "\n",
    "    # Reproject training data to the GeoMAD CRS and convert to xarray\n",
    "    training_reprojected = training_data.to_crs(final_data.odc.crs)\n",
    "    training_da = training_reprojected.assign(\n",
    "        x=training_reprojected.geometry.x, y=training_reprojected.geometry.y\n",
    "    ).to_xarray()\n",
    "\n",
    "    # Extract training values from the masked dataset\n",
    "    training_values = (\n",
    "        final_data.sel(training_da[[\"x\", \"y\"]], method=\"nearest\")\n",
    "        .squeeze()\n",
    "        .compute()\n",
    "        .to_pandas()\n",
    "    )\n",
    "\n",
    "    print(\"Finished extracting training values\")\n",
    "\n",
    "    # Join the training data with the extracted values and remove unnecessary columns\n",
    "    training_array = pd.concat([training_data[\"cc_id\"], training_values], axis=1)\n",
    "\n",
    "    # Drop rows where there was no data available\n",
    "    training_array = training_array.dropna()\n",
    "\n",
    "    # training_array.drop(columns=[\"spatial_ref\", \"x\", \"y\"], inplace=True)\n",
    "\n",
    "    # Preview our resulting training array\n",
    "    return training_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a580b2-f7cf-444c-bcd9-fb87088c4f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from odc.geo.geom import BoundingBox\n",
    "import os\n",
    "\n",
    "masked = False\n",
    "\n",
    "all_training_data = []\n",
    "with DaskClient(n_workers=2, threads_per_worker=16, memory_limit='12GB') as dc:\n",
    "    print(dc.dashboard_link)\n",
    "    for site_name, group in training_data_all.groupby(\"site_name\"):\n",
    "        print(f\"PROCESSING: {site_name}\")\n",
    "        out_file = f\"training-data/{site_name}{'' if masked else '-unmasked'}-training.csv\"\n",
    "        extent = group.total_bounds\n",
    "        bbox = BoundingBox(*extent, crs=\"epsg:4326\")\n",
    "        print(f\"Area is: {bbox.polygon.to_crs('epsg:6933').area / 10000:.2f} ha\")\n",
    "\n",
    "        if os.path.exists(out_file):\n",
    "            print(f\"Skipping {site_name} as {out_file} already exists\")\n",
    "            training_data = gpd.read_file(out_file)\n",
    "        else:\n",
    "            training_data = process_site(site_name, group, mask=masked)\n",
    "\n",
    "        all_training_data.append(training_data)\n",
    "        training_data.to_csv(out_file, index=False)\n",
    "\n",
    "# Combine all training data into a single DataFrame\n",
    "combined_training_data = pd.concat(all_training_data, axis=0)\n",
    "print(len(combined_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the combined training data\n",
    "combined_training_data.to_csv(f\"training-data/combined{'' if masked else '-unmasked'}-training.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b5cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
